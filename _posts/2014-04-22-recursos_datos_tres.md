---
layout: post
title: Recursos esenciales para el periodista de datos (3). La limpieza de la información
excerpt: "El periodismo de datos se ha convertido en una de las grandes oportunidades para esta profesión. Los grandes proyectos requieren una importante inversión, pero unos conocimientos básicos, interés y herramientas relativamente sencillas permiten realizar trabajos de gran valor periodístico."
author:
  name: Félix Arias
  twitter: cibermensaje
  gplus:  
  bio: Coordinador Nuevas Narrativas
  image: flx.webp
  link: https://twitter.com/cibermensaje
---
![image alt text]({{ site.baseurl }}/images/shots/innovacion_periodismo.jpeg)

El periodismo de datos se ha convertido en una de las grandes oportunidades para esta profesión. Los grandes proyectos requieren una importante inversión, pero unos conocimientos básicos, interés y herramientas relativamente sencillas permiten realizar trabajos de gran valor.

Eso es lo que [Antonio Delgado](https://twitter.com/adelgado "Perfil de este profesional en Twitter") demostró en el taller de periodismo de datos que impartió en el módulo de [Nuevas narrativas](https://twitter.com/search?q=%23NuevasNarrativas&src=typd "Hashtag en Twitter de esta asignatura") del Máster de Innovación en Periodismo ([MIP](http://mip.umh.es/ "Página de inicio de este proyecto académico")) de la UMH. Aquí, como complemento o aperitivo, se apuntan algunos de los recursos esenciales para empezar a desenvolverse en esta innovadora especialidad.

Una vez [localizada](mip.umh.es/blog/2014/04/08/recursos_datos/) y [_scrapeada_](mip.umh.es/blog/2014/04/08/recursos_datos_dos/) la información, comienza el proceso de depuración de datos. Este paso constituye uno de los más laboriosos y, al mismo, de los más determinantes para la correcta interpretación de los datos que después se quieren analizar y visualizar.

Cuando se captura información de una web y, sobre todo, de un pdf, lo más probable es que los campos de texto y de cifras presenten irregularidades e incorrecciones. El proceso de depuración puede realizarse manualmente y caso a caso, pero esto puede resultar muy laborioso e incluso inoperante cuando se trabaje con un gran número de datos.  La principal herramienta para este cometido es [Open Refine](http://openrefine.org/ "Web inicial de este producto") (antes, Google Refine).

Algunas de las principales operaciones a llevar a cabo son las siguientes:

* Eliminar espacios consecutivos, al principio o al final de los campos.
* Separar o unir columnas.
* Filtrar y unificar datos.
* Corregir erratas y agrupar registros a través de métodos como el _clustering_. Probablemente, ésta es la capacidad más potente de Open Refine.

Para hacerse una idea sobre este proceso, es posible ver la demostración que [David Cabo](https://twitter.com/dcabo "Perfil de este desarrollador informático en Twitter") realizó en la [Segunda sesión formativa de periodismo de datos: Análisis y tratamiento de datos](http://medialab-prado.es/article/sesion_formativa_periodismodatos_analisis_tratamiento_datos "Resumen y programa de esta actividad") organizada por el [Grupo de Periodismo de datos](http://medialab-prado.es/article/periodismo_de_datos_-_grupo_de_trabajo "Sección de esta iniciativa") del [Medialab Prado](http://medialab-prado.es/ "Web inicial de esta iniciativa") de Madrid.

Una vez depurados los datos, ya solo restan otros dos pasos:

* El análisis de la información (4).
* La visualización de los datos (5).

Todos ellos se irán desgranando, en breve y poco a poco, en [este blog](http://mip.umh.es/blog/ "Web inicial de este proyecto").